%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% input_validation.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Partial Disappearance Event Classification}
Unlike the total disappearance region, where rejecting standalone muons and limiting calorimeter energy is sufficient to control muon backgrounds, the partial disappearance region must separate potential signal events based on a large number of input features, often with subtle correlations between them.
The primary distinguishing feature of signal events in this region is a large mismatch in the energy of the nearest standalone muon and the selected probe track, but the requirement that the standalone muon have less than 30$\%$ of the probe track still leaves several thousand background events.

Primarily caused by poorly reconstructed standalone muons, these backgrounds often have poorer fit qualities, CSC hits closer to the projected track, and larger HCAL energy deposits.
These effects are not fully independent.
For instance, muons with large HCAL energy may have genuine track deflection from SM scattering, so their fit qualities will be better.
Similarly, muons with poor quality fits will often have several missing CSC hits, allowing poor standalone muons with large energy differences to be reconstructed.

Signal events often have differences in the $\phi$ of the standalone muon which are correlated with the track charge, as the lower energy deflected muons have additional curvature in the magnetic field.
Signal events may also have significant CSC hit offsets due to the angular scatter caused by \dbrem, or missing energy in several HCAL depths from muons deflecting outside of their acceptance.

To take advantage of the wide variety of weak input separators available in this region, a Boosted Decision Tree (BDT) classifier is used to reduce these inputs to one classification variable.

\subsection{Boosted Decision Trees}
Boosted decision trees are a type of supervised machine learning where separate additive functions are used to map a set of inputs to a single output.
Each individual function consists of a single 'tree', where input data is split recursively over a series of 'depths'. 
The depths are made of 'nodes', where the data is split based on a selection applied to some input variables, and 'leaves', a terminal value of the tree where a value is output.
Each node is chosen to optimize the information gained by the split made, and the tree continues until the maximum depth is reached or no further information is gained.

The "boosting" originates from the method used to produce many input trees and combine their outputs \cite{GradBoost}. 
As it is normally impossible to create all potential tree structures, trees are generated iteratively by starting from a single leaf and adding branches which minimize the "loss function", which is some function used to quantify the distance between the true and predicted value.
In this analysis, the loss function is chosen to be a binary logistic function,

\begin{equation}
	\label{binLogFunc}
	L = - \frac{1}{N} \sum_{i=1}^{N} y_i log(p(y_i)) + (1-y_i) log(1-p(y_i))
\end{equation} 

where N is the total number of training events, $y_i$ is the true value of each point with zero for background and one for signal, and $p(y_i)$ is the output score for that point.
The binary logistic loss function is small for predictions which are close to the true value, and increases exponentially for predictions which are far from the true value to increase the penalty for incorrect classification.

Each tree is then given a weight according to its accuracy and combined to produce a continuous output score from zero to one, with events near zero being the most background-like and events near one being the most signal-like. 
After each step of optimizing the tree weights, the training data is re-weighted to increase the importance of events that are more frequently misclassified in order to more quickly approach the strongest classification.
The tree optimization and re-weighting is then continued for several iterations in order to minimize an objective function consisting of the loss function and a regularization function, which reduces overtraining by penalizing the complexity of the trees used.

The main advantage of tree boosting is that it allows many weak classifiers in the individual trees to be combined into one strong classifier using the ensemble output score.
In this analysis, a "Gradient" BDT was used, provided by the XGBoost framework \cite{XGBoost} interfaced with Scikit-learn \cite{scikit}. 

\subsection{Inputs}
There are 21 event variables which are used as inputs to the BDT, which can be grouped into 5 rough categories.

The first are variables related to the probe track kinematics. 
The $p_t$, $\eta$, $\phi$, and charge of the probe track are used in order to allow the network to encode effects which may be local to certain regions of the detector.
As the CSCs have known gaps in particular depths and regions of poor reconstruction, these variables can help identify tracks which may be expected to have lower quality.

Second are the standalone muon kinematics, specifically its energy and $\chi^{2}$.
The $\chi^{2}$ of the standalone muon fit is used to measure its quality, and helps to separate muons with genuine energy loss from those with poor fits.
The overall standalone muon energy is used in addition to the difference in standalone muon and probe track energy in case there are energy-dependent factors that may be lost when using only the relative measure.

The third category are the CSC segment distances.
In each CSC station, the minimum $\Delta$R is measured from each CSC segment to the projected probe track at its position of closest approach to that segment.
Signal events often see finite differences in these CSC hit differences, while background events often have very close CSC hits in every station or have several stations with no nearby segments resulting in a poor fit and significant mismeasured energy.

Fourth, the difference between the standalone muon and the probe track in energy, $\Delta$R, and $\Delta \phi$ are used.
Signal events are have significantly lower energy standalone muons from energy loss to the \aprime, as well as larger $\Delta$R from angular deflection in \dbrem and additional curvature in the magnetic field.
To help distinguish angular changes caused by energy loss, the $\Delta \phi$ between the standalone muon and probe track is multiplied by the track charge, as differences due to random scatters and poor reconstruction should have no correlation between the charge and $\Delta \phi$.

Lastly, the HCAL energy in each depth along the probe trajectory is used to identify events with high energy deposits corresponding to SM processes, or multiple low energy deposits corresponding to potential signal events.
The number of depths traversed by any given muon depends on its position in HCAL.
To accomodate this, projected tracks which do not pass through a given depth in HCAL are assigned an energy of -1. 

\subsection{Optimization}
The primary challenge with training the partial region BDT is the imbalance between signal and background events. 
Because \dbrem is expected to be a rare process, there are should be many more background events than signal events in the data sample, and background rejection must be strongly prioritized over signal purity.

To represent this, signal events are given reduced weights to replicate their lower relative rate and increase the importance of background rejection.
To reduce the necessary validation and simplify the network, each signal mass is combined into one training input and weighted to give equal importance to each mass.

The internal parameters of the BDT are chosen using a randomized search, where each parameter is sampled from a flat distribution of possible values for a large number of trials.
To reduce the impact of overtraining in this parameter choosing, the training data is split into five randombly sampled independent subsets called \emph{folds}.
Five separate testing and training subsamples are then defined, each with the test set as one fold and the training set as the other four.

Five separate BDTs are then trained using these datasets, and the average weighted mean squared error of the five BDTs is used as the figure of merit in the randomized parameter search.
By using the parameters with the average mean squared error, the optimum parameters can be selected by using the entire dataset to test the performance, as every event has a BDT which did not use it as training data, reducing the potential bias caused by the testing selection.

The parameters which are optimized are the number of trees, the maximum tree depth, the fraction of each training sample used, the weighting factor of each new tree, the loss reduction requred to create new leaves, and fraction of features used in each tree, the minimum weight in each node, and the strength of the regularization terms used in the objective function.


\subsection{Validation}

